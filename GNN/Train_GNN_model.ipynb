{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import seed_everything, LoadhERGDataset\n",
    "from config import SEED_NO, NUM_FEATURES, NUM_GRAPHS_PER_BATCH, NUM_TARGET, EDGE_DIM, DEVICE, PATIENCE, EPOCHS, N_SPLITS, params_vertical_gnn\n",
    "from engine import EnginehERG\n",
    "from model import VerticalGNN\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from torch_geometric.loader import DataLoader\n",
    "import os "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tuning(train_loader, valid_loader, params):\n",
    "    model = VerticalGNN(num_features=NUM_FEATURES, num_targets=NUM_TARGET, num_gin_layers=params['num_gin_layers'], num_graph_trans_layers=params['num_graph_trans_layers'], \n",
    "                            hidden_size=params['hidden_size'], n_heads=params['n_heads'], dropout=params['dropout'], edge_dim=EDGE_DIM)\n",
    "    model.to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr = params['learning_rate'])\n",
    "    eng = EnginehERG(model, optimizer, device=DEVICE)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter = PATIENCE\n",
    "    early_stopping_counter = 0 \n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        valid_loss_tuple = eng.validate(valid_loader)\n",
    "        valid_loss = valid_loss_tuple[0]\n",
    "        print(f'Epoch: {epoch+1}/{EPOCHS}, train loss : {train_loss}, validation loss : {valid_loss}')\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss \n",
    "            early_stopping_counter=0\n",
    "\n",
    "        else:\n",
    "            early_stopping_counter +=1\n",
    "\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            print('Early stopping...')\n",
    "            break\n",
    "        print(f'Early stop counter: {early_stopping_counter}')\n",
    "    \n",
    "    return best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'num_gin_layers' : trial.suggest_categorical('num_gin_layers', [1, 2, 3]),\n",
    "        'num_graph_trans_layers' : trial.suggest_categorical('num_graph_trans_layers', [1, 2, 3]),\n",
    "        'hidden_size' : trial.suggest_categorical('hidden_size', [64, 128, 256]),\n",
    "        'n_heads' : trial.suggest_categorical('n_heads', [1, 2, 3]),\n",
    "        'dropout': trial.suggest_categorical('dropout', [0.1, 0.2, 0.3, 0.4]),\n",
    "        'learning_rate' : trial.suggest_categorical('learning_rate', [1e-3, 3e-3, 5e-3, 7e-3, 9e-3])\n",
    "    }\n",
    "    \n",
    "    \n",
    "    #load dataset \n",
    "    dataset_for_cv = LoadhERGDataset(root='./data/graph_data/data_hERG_train/', raw_filename='data_hERG_train.csv')\n",
    "    kf = KFold(n_splits=N_SPLITS)\n",
    "    fold_loss = 0\n",
    "\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        print(f'Fold {fold_no}')\n",
    "        train_dataset= []\n",
    "        valid_dataset = []\n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(torch.load(f'./data/graph_data/data_hERG_train/processed/molecule_{t_idx}.pt'))\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(torch.load(f'./data/graph_data/data_hERG_train/processed/molecule_{v_idx}.pt'))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False)\n",
    "\n",
    "        loss = run_tuning(train_loader, valid_loader, params)\n",
    "        fold_loss += loss\n",
    "\n",
    "    return fold_loss/10\n",
    "if __name__ == '__main__':\n",
    "    study = optuna.create_study(direction = 'minimize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(f'best trial:')\n",
    "    trial_ = study.best_trial\n",
    "    print(trial_.values)\n",
    "    print(f'Best parameters: {trial_.params}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/validate/test model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_loader, valid_loader, params, trained_model_path):\n",
    "    model = VerticalGNN(num_features=NUM_FEATURES, num_targets=NUM_TARGET, num_gin_layers=params['num_gin_layers'], num_graph_trans_layers=params['num_graph_trans_layers'], \n",
    "                            hidden_size=params['hidden_size'], n_heads=params['n_heads'], dropout=params['dropout'], edge_dim=EDGE_DIM)\n",
    "    model.to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr = params['learning_rate'])\n",
    "    eng = EnginehERG(model, optimizer, device=DEVICE)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter = PATIENCE\n",
    "    early_stopping_counter = 0 \n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        valid_loss_tuple = eng.validate(valid_loader)\n",
    "        valid_loss = valid_loss_tuple[0]\n",
    "        print(f'Epoch: {epoch+1}/{EPOCHS}, train loss : {train_loss}, validation loss : {valid_loss}')\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss \n",
    "            early_stopping_counter=0 #reset counter\n",
    "            print('Saving model...')\n",
    "            \n",
    "            os.makedirs(os.path.dirname(trained_model_path), exist_ok=True)\n",
    "\n",
    "            torch.save(model.state_dict(), trained_model_path)\n",
    "        else:\n",
    "            early_stopping_counter +=1\n",
    "\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            print('Early stopping...')\n",
    "            break\n",
    "        print(f'Early stop counter: {early_stopping_counter}')\n",
    "    \n",
    "    return best_loss\n",
    "\n",
    "def run_validation(valid_loader, params, trained_model_path):\n",
    "    model = VerticalGNN(num_features=NUM_FEATURES, num_targets=NUM_TARGET, num_gin_layers=params['num_gin_layers'], num_graph_trans_layers=params['num_graph_trans_layers'], \n",
    "                            hidden_size=params['hidden_size'], n_heads=params['n_heads'], dropout=params['dropout'], edge_dim=EDGE_DIM)\n",
    "    model.load_state_dict(torch.load(trained_model_path))\n",
    "    model.to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr = params['learning_rate'])\n",
    "    eng = EnginehERG(model, optimizer, device=DEVICE)\n",
    "    mse, r2 = eng.validate(valid_loader)\n",
    "    print(f\"mse :{mse}, r2:{r2}\")\n",
    "    return mse, r2 \n",
    "\n",
    "\n",
    "def run_testing(test_loader, params, trained_model_path):\n",
    "    model = VerticalGNN(num_features=NUM_FEATURES, num_targets=NUM_TARGET, num_gin_layers=params['num_gin_layers'], num_graph_trans_layers=params['num_graph_trans_layers'], \n",
    "                            hidden_size=params['hidden_size'], n_heads=params['n_heads'], dropout=params['dropout'], edge_dim=EDGE_DIM)\n",
    "    model.load_state_dict(torch.load(trained_model_path))\n",
    "    model.to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr = params['learning_rate'])\n",
    "    eng = EnginehERG(model, optimizer, device=DEVICE)\n",
    "\n",
    "    mse, r2 = eng.test(test_loader)\n",
    "    print(f\"mse :{mse}, r2:{r2}\")\n",
    "    return mse, r2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params_vertical_gnn\n",
    "n_repetitions = 1\n",
    "train_data_root_path = './data/graph_data/data_hERG_train/'\n",
    "train_data_raw_filename = 'data_hERG_train.csv'\n",
    "test_data_root_path = './data/graph_data/data_hERG_test/'\n",
    "test_data_raw_filename = 'data_hERG_test.csv'\n",
    "path_to_save_trained_model = './trained_models/vertical/'\n",
    "\n",
    "val_mse_list = []\n",
    "val_r2_list = []\n",
    "\n",
    "mse_list = []\n",
    "r2_list = []\n",
    "\n",
    "\n",
    "dataset_for_cv = LoadhERGDataset(root=train_data_root_path, raw_filename=train_data_raw_filename)\n",
    "test_dataset = LoadhERGDataset(root=test_data_root_path, raw_filename=test_data_raw_filename)\n",
    "kf = KFold(n_splits=N_SPLITS)\n",
    "\n",
    "for repeat in range(n_repetitions):\n",
    "    repeat_val_r2_list = []\n",
    "    repeat_val_mse_list = []\n",
    "    \n",
    "    repeat_r2_list = []\n",
    "    repeat_mse_list = []\n",
    "\n",
    "    for fold_no, (train_idx, valid_idx) in enumerate(kf.split(dataset_for_cv)):\n",
    "        seed_everything(SEED_NO)\n",
    "        train_dataset= []\n",
    "        valid_dataset = []\n",
    "        for t_idx in train_idx:\n",
    "            train_dataset.append(torch.load(f'./data/graph_data/data_hERG_train/processed/molecule_{t_idx}.pt'))\n",
    "        for v_idx in valid_idx:\n",
    "            valid_dataset.append(torch.load(f'./data/graph_data/data_hERG_train/processed/molecule_{v_idx}.pt'))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=False)\n",
    "\n",
    "        run_training(train_loader, valid_loader, params, os.path.join(path_to_save_trained_model, f'vertical_repeat_{repeat}_fold_{fold_no}.pt'))\n",
    "        val_mse, val_r2 = run_validation(valid_loader, params, os.path.join(path_to_save_trained_model, f'vertical_repeat_{repeat}_fold_{fold_no}.pt'))\n",
    "        mse, r2 = run_testing(test_loader, params, os.path.join(path_to_save_trained_model, f'vertical_repeat_{repeat}_fold_{fold_no}.pt'))\n",
    "        \n",
    "        repeat_val_mse_list.append(val_mse)\n",
    "        repeat_val_r2_list.append(val_r2)\n",
    "        \n",
    "        repeat_mse_list.append(mse)\n",
    "        repeat_r2_list.append(r2)\n",
    "        \n",
    "        val_mse_list.append(val_mse)\n",
    "        val_r2_list.append(val_r2)\n",
    "        \n",
    "        mse_list.append(mse)\n",
    "        r2_list.append(r2)\n",
    "        \n",
    "\n",
    "    print(f'Statistics for repeat {repeat}:')\n",
    "    print(f'Validation - mse: {np.mean(repeat_val_mse_list):.3f}±{np.std(repeat_val_mse_list):.3f}')\n",
    "    print(f'Validation - r2: {np.mean(repeat_val_r2_list):.3f}±{np.std(repeat_val_r2_list):.3f}')\n",
    "    \n",
    "    print(f'test - mse: {np.mean(repeat_mse_list):.3f}±{np.std(repeat_mse_list):.3f}')\n",
    "    print(f'test - r2: {np.mean(repeat_r2_list):.3f}±{np.std(repeat_r2_list):.3f}')\n",
    "    \n",
    "val_mse_arr = np.array(val_mse_list)\n",
    "val_mse_mean= np.mean(val_mse_arr)\n",
    "val_mse_sd = np.std(val_mse_arr)\n",
    "print(f'validation mse:{val_mse_mean:.3f}±{val_mse_sd:.3f}')\n",
    "\n",
    "val_r2_arr = np.array(val_r2_list)\n",
    "val_mean_r2 = np.mean(val_r2_arr)\n",
    "val_sd_r2 = np.std(val_r2_arr)\n",
    "print(f'validation r2:{val_mean_r2:.3f}±{val_sd_r2:.3f}')\n",
    "\n",
    "\n",
    "mse_arr = np.array(mse_list)\n",
    "mse_mean= np.mean(mse_arr)\n",
    "mse_sd = np.std(mse_arr)\n",
    "print(f'mse:{mse_mean:.3f}±{mse_sd:.3f}')\n",
    "\n",
    "r2_arr = np.array(r2_list)\n",
    "mean_r2 = np.mean(r2_arr)\n",
    "sd_r2 = np.std(r2_arr)\n",
    "print(f'r2:{mean_r2:.3f}±{sd_r2:.3f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
